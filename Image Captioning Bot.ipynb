{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "import keras\n",
    "from keras.applications.resnet50 import ResNet50, preprocess_input, decode_predictions\n",
    "from keras.preprocessing import image\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense,Input,Dropout,Embedding,LSTM\n",
    "from time import time\n",
    "import pickle\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "from keras.layers.merge import add\n",
    "import  matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"Flickr8k.token.txt\") as f:\n",
    "    captions = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "captions = captions.split(\"\\n\")[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1000268201_693b08cb0e.jpg#0\\tA child in a pink dress is climbing up a set of stairs in an entry way .'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(captions)\n",
    "captions[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "descriptions = {}\n",
    "\n",
    "for element in captions:\n",
    "    i_to_c = element.split(\"\\t\")\n",
    "    cap = i_to_c[1]\n",
    "    \n",
    "    key = i_to_c[0].split(\".\")[0]\n",
    "    \n",
    "    if descriptions.get(key) == None:\n",
    "        descriptions[key] = []\n",
    "        \n",
    "    descriptions[key].append(cap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['A child in a pink dress is climbing up a set of stairs in an entry way .',\n",
       " 'A girl going into a wooden building .',\n",
       " 'A little girl climbing into a wooden playhouse .',\n",
       " 'A little girl climbing the stairs to her playhouse .',\n",
       " 'A little girl in a pink dress going into a wooden cabin .']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "descriptions[\"1000268201_693b08cb0e\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "1. lower\n",
    "2. remove puntuations\n",
    "3. remove words less than length 2\n",
    "\"\"\"\n",
    "\n",
    "def clean_text(sample):\n",
    "    sample = sample.lower()\n",
    "    sample = re.sub(\"[^a-z]+\",\" \", sample)\n",
    "    sample = sample.split()\n",
    "    \n",
    "    sample = [s for s in sample if len(s)>1]\n",
    "    \n",
    "    sample = \" \".join(sample)\n",
    "    \n",
    "    return sample\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, desc_list in descriptions.items():\n",
    "    for i in range(len(desc_list)):\n",
    "        descriptions[key][i] = clean_text(desc_list[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['child in pink dress is climbing up set of stairs in an entry way',\n",
       " 'girl going into wooden building',\n",
       " 'little girl climbing into wooden playhouse',\n",
       " 'little girl climbing the stairs to her playhouse',\n",
       " 'little girl in pink dress going into wooden cabin']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "descriptions[\"1000268201_693b08cb0e\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8424\n"
     ]
    }
   ],
   "source": [
    "# create unique vocabulary\n",
    "\n",
    "vocabulary = set()\n",
    "\n",
    "for key in descriptions.keys():\n",
    "    [vocabulary.update(i.split()) for i in descriptions[key]]\n",
    "    \n",
    "print(len(vocabulary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "373837\n"
     ]
    }
   ],
   "source": [
    "all_vocab = []\n",
    "\n",
    "for key in descriptions.keys():\n",
    "    [all_vocab.append(i) for des in descriptions[key] for i in des.split()]\n",
    "    \n",
    "print(len(all_vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter = collections.Counter(all_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_ = dict(counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_dic = sorted(dict_.items(),reverse=True,key = lambda x : x[1])\n",
    "threshhold_val = 10\n",
    "sorted_dic = [x for x in sorted_dic if x[1]>threshhold_val]\n",
    "vocabulary = [x[0] for x in sorted_dic]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1845"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocabulary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Training Testing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"Flickr_8k.trainImages.txt\") as f:\n",
    "    train = f.read()\n",
    "with open(\"Flickr_8k.testImages.txt\") as f:\n",
    "    test = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = [e.split(\".\")[0] for e in train.split(\"\\n\")[:-1]]\n",
    "test = [e.split(\".\")[0] for e in test.split(\"\\n\")[:-1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_descriptions = {}\n",
    "for t in train:\n",
    "    train_descriptions[t] = []\n",
    "    \n",
    "    for cap in descriptions[t]:\n",
    "        cap_append = \"startseq \" + cap + \" endseq\"\n",
    "        train_descriptions[t].append(cap_append)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['startseq child in pink dress is climbing up set of stairs in an entry way endseq',\n",
       " 'startseq girl going into wooden building endseq',\n",
       " 'startseq little girl climbing into wooden playhouse endseq',\n",
       " 'startseq little girl climbing the stairs to her playhouse endseq',\n",
       " 'startseq little girl in pink dress going into wooden cabin endseq']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_descriptions[\"1000268201_693b08cb0e\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ResNet50(weights = \"imagenet\",input_shape=(224,224,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_new = Model(inputs = model.input,outputs=model.layers[-2].output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = \"flickr_data/Flickr_Data/Images/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_image(img):\n",
    "    img = image.load_img(img, target_size=(224,224))\n",
    "    img = image.img_to_array(img)\n",
    "    img = np.expand_dims(img,axis=0)\n",
    "    img = preprocess_input(img)\n",
    "    \n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_image(img):\n",
    "    img = preprocess_image(img)\n",
    "    feature_vector = model_new.predict(img).reshape((2048,))\n",
    "    return feature_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = encode_image(images+\"1000268201_693b08cb0e.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2048,)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoding image 5900\r"
     ]
    }
   ],
   "source": [
    "start = time()\n",
    "\n",
    "encoding_train = {}\n",
    "\n",
    "for ix,img_ in enumerate(train):\n",
    "    \n",
    "    img = images+img_+\".jpg\"\n",
    "    \n",
    "    encoding_train[img] = encode_image(img)\n",
    "    \n",
    "    if ix%100 ==0:\n",
    "        print(\"encoding image \"+str(ix),end=\"\\r\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# writing pkl file on disk\n",
    "with open(\"encoded_train_images.pkl\", \"wb\") as p:\n",
    "    pickle.dump(encoding_train,p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reading pkl file from disk\n",
    "with open(\"encoded_train_images.pkl\", \"rb\") as p:\n",
    "    encoding_train = pickle.load(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time()\n",
    "\n",
    "encoding_test = {}\n",
    "\n",
    "for ix,img_ in enumerate(test):\n",
    "    \n",
    "    img = images+img_+\".jpg\"\n",
    "    \n",
    "    encoding_test[img] = encode_image(img)\n",
    "    \n",
    "    if ix%100 ==0:\n",
    "        print(\"encoding image \"+str(ix),end=\"\\r\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# writing pkl file on disk\n",
    "with open(\"encoded_test_images.pkl\", \"wb\") as p:\n",
    "    pickle.dump(encoding_test,p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reading pkl file from disk\n",
    "with open(\"encoded_test_images.pkl\", \"rb\") as p:\n",
    "    encoding_test = pickle.load(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing Captions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mapping word_to_idx and idx_to_word\n",
    "\n",
    "ix = 1\n",
    "word_to_idx = {}\n",
    "idx_to_word = {}\n",
    "\n",
    "for e in vocabulary:\n",
    "    word_to_idx[e] = ix\n",
    "    idx_to_word[ix] = e\n",
    "    ix+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_idx['dog']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_to_word[1846] = \"startseq\"\n",
    "word_to_idx[\"startseq\"] = 1846\n",
    "\n",
    "idx_to_word[1847] = \"endseq\"\n",
    "word_to_idx[\"endseq\"] = 1847"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(idx_to_word) +1\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_caption_len = []\n",
    "\n",
    "for key in train_descriptions.keys():\n",
    "    for cap in train_descriptions[key]:\n",
    "        all_caption_len.append(len(cap.split()))\n",
    "        \n",
    "max_len = max(all_caption_len)\n",
    "print(max_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Prepare using Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_generator(train_descriptions,encoding_train,word_to_idx, max_len,num_photos_per_batch):\n",
    "    x1,x2,y = [],[],[]\n",
    "    n=0\n",
    "    while True:\n",
    "        for key, desc_list in train_descriptions.items():\n",
    "            n+=1\n",
    "            photo = encoding_train[images+key+\".jpg\"]\n",
    "            for desc in desc_list:\n",
    "                seq = [word_to_idx[word] for word in desc.split() if word in word_to_idx]\n",
    "                for i in range(1,len(seq)):\n",
    "                    in_seq = seq[0:i]\n",
    "                    out_seq = seq[i]\n",
    "                    in_seq = pad_sequences([in_seq],maxlen=max_len,value=0,padding=\"post\")[0]\n",
    "                    out_seq = to_categorical([out_seq],num_classes=vocab_size)[0]\n",
    "                    x1.append(photo)\n",
    "                    x2.append(in_seq)\n",
    "                    y.append(out_seq)\n",
    "            if n==num_photos_per_batch:\n",
    "                yield [[np.array(x1),np.array(x2)],np.array(y)]\n",
    "                x1,x2,y =[],[],[]\n",
    "                n=0\n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"glove.6B.50d.txt\",encoding=\"utf8\") as f:\n",
    "    embedding_index={}\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:],dtype=\"float\")\n",
    "        embedding_index[word] = coefs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedding_output():\n",
    "    em_dim = 50\n",
    "    \n",
    "    embedding_output = np.zeros((vocab_size, em_dim))\n",
    "    \n",
    "    for word, idx in word_to_idx.items():\n",
    "        embedding_vector = embedding_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_output[idx] = embedding_vector\n",
    "    return embedding_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_output = get_embedding_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(embedding_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Arch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# image feature extractor model\n",
    "input_img_fea = Input(shape=(2048,))\n",
    "inp_img1 = Dropout(0.3)(input_img_fea)\n",
    "inp_img2 = Dense(256, activation=\"relu\")(inp_img1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# partial caption sequence model\n",
    "inp_cap = Input(shape=(max_len,))\n",
    "inp_cap1 = Embedding(input_dim=vocab_size,output_dim=50,mask_zero=True)(inp_cap)\n",
    "inp_cap2 = Dropout(0.3)(inp_cap1)\n",
    "inp_cap3 = LSTM(256)(inp_cap2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combining both models\n",
    "\n",
    "decoder = add([inp_img2,inp_cap3])\n",
    "decoder1 = Dense(256,activation=\"relu\")(decoder)\n",
    "output = Dense(vocab_size,activation=\"softmax\")(decoder1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(inputs = [input_img_fea,inp_cap],outputs = output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.layers[2].set_weights([embedding_output])\n",
    "model.layers[2].trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=\"categorical_crossentropy\", optimizer = \"adam\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 10\n",
    "number_pics_per_batch = 3\n",
    "steps = len(train_descriptions)//number_pics_per_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(epochs):\n",
    "    generator = data_generator(train_descriptions, encoding_train,word_to_idx,max_len,number_pics_per_batch)\n",
    "    model.fit_generator(generator,epochs=1,steps_per_epoch=steps)\n",
    "    model.save(\"model_\"+str(i)+\".h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_caption(photo):\n",
    "    in_text = \"startseq\"\n",
    "    \n",
    "    for i in range(max_len):\n",
    "        sequence = [word_to_idx[w] for w in in_text.split() if w in word_to_idx]\n",
    "        sequence = pad_sequences([sequence], maxlen=max_len, padding='post')\n",
    "\n",
    "        ypred =  model.predict([photo,sequence])\n",
    "        ypred = ypred.argmax()\n",
    "        word = idx_to_word[ypred]\n",
    "        in_text+= ' ' +word\n",
    "        \n",
    "        if word =='endseq':\n",
    "            break\n",
    "        \n",
    "        \n",
    "    final_caption =  in_text.split()\n",
    "    final_caption = final_caption[1:-1]\n",
    "    final_caption = ' '.join(final_caption)\n",
    "    \n",
    "    return final_caption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(20):\n",
    "    rn =  np.random.randint(0, 1000)\n",
    "    img_name = list(encoding_test.keys())[rn]\n",
    "    photo = encoding_test[img_name].reshape((1,2048))\n",
    "\n",
    "    i = plt.imread(img_name)\n",
    "    plt.imshow(i)\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n",
    "\n",
    "    caption = predict_caption(photo)\n",
    "    print(caption)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
